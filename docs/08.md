{% include lib/mathjax.html %}
# Fast Gradient Sign Method

## จากแบบจำลองเชิงเส้นสู่ deep learning
หลังจากที่เราได้รู้จักการสร้าง adversarial example และการทำ robust optimization ในแบบจำลองที่เป็นเชิงเส้นกันมาแล้ว 
เรามาลองพิจารณาบนแบบจำลอง deep neural network กันดูบ้าง ปัญหาแรกที่เราสนใจคือปัญหาการสร้าง adversarial example
สำหรับตัวอย่างข้อมูล $$(x,y)$$ ใด ๆ ซึ่งมองเป็นปัญหา optimization ได้ดังนี้

$$
\max_{\delta\in\Delta}\ell(h_\theta(x+\delta),y)
$$

โดยในคราวนี้เราจะให้ $$h_\theta:\mathbb{R}^n\rightarrow\mathbb{R}^k$$ แทนแบบจำลอง deep neural network ที่มี $$d$$ layer ซึ่งสามารถอธิบายได้ด้วยสมการต่อไปนี้

$$
\begin{split}
z_1&=&x\\
z_{i+1}&=&f_i(W_iz_i+b_i), \text{ สำหรับ } i=1,\dots,d\\
h_\theta(x)&=&z_{d+1}
\end{split}
$$

เมื่อ $$z_i$$ แทนค่าที่จะถูกส่งเข้าไปคำนวณใน layer ที่ $$i$$ ซึ่งมี activation function เป็น $$f_i$$ โดย activation function ที่นิยมในปัจจุบันคือ ReLU $$f_i(z)=\max\{0,z\}$$ สำหรับ layer $$i=1,\dots,d-1$$ และ $$f_d(z)=z$$ สำหรับ parameter ที่ถูกปรับค่าระหว่างการเทรนนั้นคือ $$\theta=\{W_1,b_1,\dots,W_d,b_d\}$$ และ loss function ที่เราใช้ในการเทรนก็คือ softmax cross entropy

$$
\ell(h_\theta(x),y)=\log\sum_{j=1}^ke^{h_\theta(x)_j}-h_\theta(x)_y
$$

ในกรณีของ deep neural network นั้นการแก้ปัญหา maximization สำหรับสร้าง adversarial example ไม่สามารถทำได้ง่ายเหมือนแบบจำลองเชิงเส้น เนื่องจากลักษณะของ loss function เป็น non-convex เราจึงสนใจการประมาณค่าให้ได้ใกล้เคียงที่สุด

## Fast Gradient Sign Method
หนึ่งในอัลกอริทึมกลุ่มแรกที่ถูกเสนอสำหรับสร้าง adversarial example บนแบบจำลอง deep learning นี้เรียกว่า Fast Gradient Sign Method หรือ FGSM

พิจารณาปัญหา maximization เมื่อขอบเขตการก่อกวนอยู่ในรูปของ $$\ell_\infty$$-norm $$\|\delta\|_\infty\leq\epsilon$$ ลักษณะขอบเขตของการก่อกวนสำหรับตัวอย่างข้อมูล $$x$$ แสดงได้ดังรูปด้านล่าง


