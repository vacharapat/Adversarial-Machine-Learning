{% include lib/mathjax.html %}
# Adversarial training

หลังจากที่เราได้รู้จักวิธีสร้าง adversarial example กันมาแล้ว คราวนี้เราจะกลับมาพิจารณาการสร้างแบบจำลอง machine learning 
ที่มีความทนทานต่อการก่อกวนเหล่านี้ จากที่เราได้ทราบกันมาแล้วว่า สำหรับแบบจำลอง $$h_\theta$$ ใด ๆ หากเราต้องการหาพารามิเตอร์ $$\theta$$
ที่ทำให้แบบจำลองของเราทนทานต่อการก่อกวนมากที่สุด เราจะต้องหาค่า $$\theta$$ ที่นำไปสู่ adverarial risk ที่น้อยที่สุด นั่นคือ เราต้องการค่า $$\theta$$ ที่

$$
\mathbb{E}_{(x,y)\sim\mathcal{D}}[\max_{\delta\in\Delta(x)}\ell(h_\theta(x+\delta),y)]
$$

มีค่าน้อยที่สุด ซึ่งในทางปฏิบัติ เมื่อเรามีชุดข้อมูล $$D=\{(x_1,y_1),\dots,(x_m,y_m)\}$$ เราสนใจการหาค่า $$\theta$$ ที่มีค่าเฉลี่ยของ adversarial loss ของชุดข้อมูลนี้น้อยที่สุด ดังนั้น ปัญหาในการเทรนแบบจำลองของเราสามารถเขียนเป็นปัญหา optimization ได้เป็น

$$
\min_\theta\frac{1}{m}\sum_{i=1}^m \max_{\delta_i\in\Delta(x_i)}\ell(h_\theta(x_i+\delta_i),y_i)
$$

แต่จากที่เราได้ทราบแล้ว การหาค่า $$\theta$$ ที่ต้องการนี้โดยใช้แนวคิดของการทำ gradient descent มีปัญหาตรงที่เราจะหา gradient ของค่าเฉลี่ย adversarial loss นี้ได้ก็ต่อเมื่อเราต้องสามารถแก้ปัญหา maximization ด้านในให้ได้เท่านั้น เนื่องจาก Danskin's theorem ได้แสดงว่า ถ้า

$$
\delta^*=\arg\max_{\delta\in\Delta(x)}\ell(h_\theta(x+\delta),y)
$$

เราจะได้ว่า

$$
\nabla_\theta\max_{\delta\in\Delta(x)}\ell(h_\theta(x+\delta),y) = \nabla_\theta\ell(h_\theta(x+\delta^*),y)
$$

<p align="center">
<img width="300" src="https://raw.githubusercontent.com/vacharapat/Adversarial-Machine-Learning/master/images/pgd_result.png">
</p>

