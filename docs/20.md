{% include lib/mathjax.html %}
# Generalization bound
หลังจากที่เราได้ทำการวิเคราะห์ sample complexity และ generalization bound ของ hypothesis space ที่มีขนาดจำกัดมาแล้ว
ในหัวข้อนี้เราจะมาลองวิเคราะห์สำหรับ hypothesis space ใด ๆ ดูบ้าง สังเกตว่าเราไม่สามารถใช้ขนาดของ hypothesis space $H$
เป็นตัวแทนของความซับซ้อนของ $H$ ในการวิเคราะห์ได้อีก เนื่องจาก $H$ อาจมีขนาดไม่จำกัดซึ่งจะทำให้เราคำนวณด้วยวิธีเดิมไม่ได้
อย่างไรก็ดี เราสามารถใช้ VC dimension ของ $H$ เป็นตัวแทนของความซับซ้อนของ $H$ และวิเคราะห์ generalization bound ให้อยู่ในรูปของ
VC dimension ได้

## consistent hypothesis
ให้ $H$ เป็น hypothesis space ใด ๆ (ซึ่งอาจมีขนาดจำกัดหรือไม่ก็ได้) ถ้าให้ concept เป้าหมายเป็น $c\in H$ และ $$S=\{(x_1,y_1),\dots,(x_m,y_m)\}$$ เป็นเซตของตัวอย่างข้อมูล $m$ ตัว
หากอัลกอริทึมการเรียนรู้ของเราทำการเลือก hypothesis $h\in H$ ที่ consistent กับ $S$ ได้ เราจะวิเคราะห์โอกาสที่ $h$ ที่ได้มาจะมีความผิดพลาดสูงกว่า $\epsilon$ โดยการหาความน่าจะเป็นของเหตุการณ์ที่มี hypothesis $h\in H$ อย่างน้อยหนึ่งตัวที่ $R(h)>\epsilon$ และ $h$ consistent กับ $S$
ถ้าให้เหตุการณ์ดังกล่าวแทนด้วย $A$ เป้าหมายของเราคือการหาขอบเขตบนของ

$$
\Pr[A] = \Pr[\exists h\in H: R(h)>\epsilon \text{ และ } h \text{ consistent กับ } S]
$$

สำหรับ hypothesis $h$ ที่ $R(h)>\epsilon$ หากเรากำหนดให้ $r$ เป็นพื้นที่ใน $X$ ที่ $h(x)\neq c(x)$ แสดงว่า $h$ จะ consistent กับ $S$ ก็ต่อเมื่อไม่มีข้อมูลตัวอย่าง $x_i$ ตัวใดตกอยู่ใน $r$ เลย สมมติว่าเราสุ่มหยิบตัวอย่างข้อมูลใน $X$ มาอีก $m$ ตัวจากการกระจายตัวแบบเดียวกัน ให้ $$S'=\{(x'_1,y'_1),\dots,(x'_m,y'_m)\}$$ เป็นเซตของตัวอย่างข้อมูลที่หยิบได้ เมื่อพิจารณา Chebyshev's inequality ซึ่งกล่าวไว้ดังนี้

#### Chebyshev's inequality
ให้ $X$ เป็นตัวแปรสุ่มที่มี standard deviation $\sigma_X$ และให้ $t$ เป็นจำนวนจริงใด ๆ ที่ $t>0$ 

$$
\Pr[|X-\mathbb{E}[X]|\geq t\sigma_X]\leq\frac{1}{t^2}
$$

ถ้าเราให้ $X$ เป็นตัวแปรสุ่มแทนจำนวนตัวอย่างข้อมูลใน $S'$ ที่ตกอยู่ในพื้นที่ $r$ เนื่องจากความน่าจะเป็นที่ตัวอย่างข้อมูลจะตกใน $r$ มีอย่างน้อย $\epsilon$ ดังนั้นเราจึงได้ว่า $\mathbb{E}[X]\geq\epsilon m$ ดังนั้น

$$
\begin{split}
\Pr[X\leq\frac{\epsilon m}{2}]&\leq\Pr[|X-\epsilon m|\geq\frac{\epsilon m}{2}]\\
&\leq \frac{4\sigma_X^2}{\epsilon^2m^2}
\end{split}
$$

เนื่องจาก $X$ มีการกระจายตัวเป็น binomial distribution ถ้าให้ $p_r$ เป็นความน่าจะเป็นที่ตัวอย่างข้อมูลจะตกในพื้นที่ $r$ เราจะได้ว่า 

$$
\sigma_X^2 = mp_r(1-p_r)
$$

เนื่องจาก $p(1-p)\leq 1/4$ สำหรับค่า $p$ ใด ๆ ที่ $0\leq p\leq 1$ เราจึงได้ว่า

$$
\Pr[X\leq\frac{\epsilon m}{2}]\leq\frac{m}{\epsilon^2m^2}=\frac{1}{\epsilon^2 m}
$$

ถ้าเรากำหนดให้ $m\geq 2/\epsilon^2$ ก็จะได้

$$
\Pr[X\leq\frac{\epsilon m}{2}]\leq\frac{1}{2}
$$

นั่นคือ เราสรุปได้ว่า หาก $m\geq 2/\epsilon^2$ ตัวอย่างข้อมูลใน $S'$ จะตกในพื้นที่ $r$ อย่างน้อย $\epsilon m/2$ ตัวด้วยความน่าจะเป็นไม่น้อยกว่า 1/2

ถ้าให้ $B$ แทนเหตุการณ์ที่มี hypothesis $h\in H$ ที่ $R(h)>\epsilon$ และ consistent กับ $S$ แต่มีตัวอย่างข้อมูลอย่างน้อย $\epsilon m/2$ ตัวใน $S'$ ที่ $h$ ให้ผลลัพธ์ไม่ถูกต้อง หากเราให้พื้นที่ที่ $h$ ตอบผิดเป็น $r$ เราอาจมองเหตุการณ์ $B$ เป็นเหตุการณ์ที่มีพื้นที่ $r$ ที่ขนาดไม่น้อยกว่า $\epsilon$ ที่ตัวอย่างข้อมูลใน $S$ ไม่ตกลงใน $r$ เลย แต่ตัวอย่างข้อมูลใน $S'$ ตกใน $r$ อย่างน้อย $\epsilon m/2$ ตัว
สังเกตว่า $\Pr[B|A]\geq 1/2$ และเนื่องจาก $B=B\cap A$ เราจึงได้ว่า

$$
\Pr[B]=\Pr[B\cap A]=\Pr[B|A]\Pr[A]\geq\frac{1}{2}\Pr[A]
$$

หรือได้ว่า $\Pr[A]\leq 2\Pr[B]$
ซึ่งทำให้เห็นว่า เราสามารถจำกัดขอบเขตของโอกาสที่เหตุการณ์ $A$ จะเกิดได้โดยการจำกัดโอกาสของเหตุการณ์ $B$ แทน

ในการหาความน่าจะเป็นของเหตุการณ์ $B$ เราทำได้โดยวิเคราะห์ความน่าจะเป็นที่จะมีพื้นที่ $r$  ที่ $r\cap S=\emptyset$ และ 
$|r\cap (S\cup S')|\geq\epsilon m/2$ แทนที่เราจะมองกระบวนการของเราเป็นการสุ่มหยิบตัวอย่างข้อมูล $S$ มา $m$ ตัว และต่อด้วยการหยิบตัวอย่างข้อมูล $S'$ อีก $m$ ตัว เราสามารถมองเป็นการสุ่มหยิบตัวอย่างข้อมูล $S_{2m}$ ทั้งหมด $2m$ ตัวแล้วจึงสุ่มแบ่งข้อมูลชุดนี้ออกเป็น $S$ และ $S'$ ภายหลัง เนื่องจากการสุ่มหยิบข้อมูลแต่ละตัวนั้นเป็น i.i.d. เราจึงได้ว่าการกระจายตัวของ $S$ และ $S'$ จากทั้งสองกระบวนการนี้จะต้องเหมือนกัน
คราวนี้พิจารณาเมื่อเราได้ตัวอย่างข้อมูล $2m$ ตัวใน $S_{2m}$ มาแล้ว ถ้าให้ $r$ เป็นพื้นที่ความผิดพลาดของบาง hypothesis ใน $H$ ที่ $|r_{|S_{2m}}|\geq \epsilon m/2$ เมื่อ $r_{|S_{2m}}=r\cap S_{2m}$ เราจะสามารถหาความน่าจะเป็นที่เมื่อแบ่ง $S_{2m}$ ออกเป็น $S$ และ $S'$ แล้ว ได้ว่า $r_{|S_{2m}}\cap S = \emptyset$ จากตรงนี้เราจะสามารถจำกัดขอบเขตความน่าจะเป็นของเหตุการณ์ $B$ ได้โดยพิจารณาทุก ๆ รูปแบบของ $r_{|S_{2m}}$ ที่เป็นไปได้ ซึ่งจะมีไม่เกิน $\Pi_H(2m)$ แบบ

ปัญหาที่เหลือของเราคือการหาความน่าจะเป็นที่เราสุ่มแบ่งข้อมูล $2m$ ตัวซึ่งอยู่ใน 
$r_{|S_{2m}}$
ไม่น้อยกว่า $\epsilon m/2$ ตัวออกเป็น $S$ และ $S'$ แล้วทำให้ $S$ ไม่มีข้อมูลที่อยู่ใน 
$r_{|S_{2m}}$
เลย เราสามารถมองปัญหานี้ใหม่ให้ง่ายขึ้นได้ว่า เรามีลูกบอลสีแดงและน้ำเงินจำนวนรวมกัน $2m$ ลูก โดยมีจำนวนลูกบอลสีแดงเท่ากับ $l\geq\epsilon m/2$ ลูก จากนั้นเราสุ่มแบ่งลูกบอลออกเป็นสองกลุ่มได้แก่ $S$ และ $S'$ จำนวนกลุ่มละ $m$ ลูก แล้วเราสนใจความน่าจะเป็นที่ลูกบอลสีแดงทั้ง $l$ ลูกนั้นอยู่ในกลุ่ม $S'$ ทั้งหมด สมมติว่าเรามีพื้นที่สำหรับลูกบอลเรียงกันทั้งสิ้น $2m$ ที่ โดยเรากำหนดให้ลูกบอล $m$ ลูกแรกจะไปอยู่ใน $S$ และ $m$ ลูกหลังจะไปอยู่ใน $S'$ ความน่าจะเป็นดังกล่าวสามารถคำนวณได้จากอัตราส่วนระหว่างจำนวนวิธีการเรียงลูกบอลโดยให้ลูกบอลสีแดงทั้ง $l$ ลูกอยู่ครึ่งทั้งหมด ต่อจำนวนวิธีการเรียงลูกบอลทั้งหมดอย่างไม่มีเงื่อนไข ซึ่งมีค่าเท่ากับ ${m\choose l}/{2m\choose l}$

เนื่องจาก

$$
\begin{split}
\frac{m\choose l}{2m\choose l}&=\frac{m!}{l!(m-l)!}\frac{l!(2m-l)!}{2m!}\\
&=\Pi_{i=0}^{l-1}\frac{m-i}{2m-i}\\
&\leq\Pi_{i=0}^{l-1}\frac{1}{2}\\
&=\frac{1}{2^l}\leq\frac{1}{2^{\epsilon m/2}}
\end{split}
$$

ดังนั้น สำหรับชุดตัวอย่างข้อมูล $S_{2m}$ ใด ๆ และพื้นที่ $r$ ที่ 
$|r_{|S_{2m}}|\geq\epsilon m/2$ ความน่าจะเป็นที่เราสุ่มแบ่งข้อมูลดังกล่าวเป็น $S$ และ $S'$ แล้ว $r_{|S_{2m}}\cap S=\emptyset$ จะมีค่าไม่เกิน
$1/2^{\epsilon m/2}$ เนื่องจากจำนวน $r_{|S_{2m}}$ จะมีได้ทั้งสิ้นไม่เกิน $\Pi_H(2m)$ ดังนั้นความน่าจะเป็นที่จะมี $r_{|S_{2m}}$ อย่างน้อยแบบหนึ่งที่ $|r_{|S_{2m}}|\geq\epsilon m/2$ และ $r_{|S_{2m}}\cap S=\emptyset$ หรือความน่าจะเป็นของเหตุการณ์ $B$ จะมีค่าดังนี้

$$
\Pr[B]\leq\Pi_H(2m)2^{-\epsilon m/2}
$$

และได้ว่า

$$
\Pr[A]\leq 2\Pr[B]\leq 2\Pi_H(2m)2^{-\epsilon m/2}\leq 2\left(\frac{2em}{d}\right)^d2^{-\epsilon m/2}
$$

เมื่อแทน $2\left(\frac{2em}{d}\right)^d2^{-\epsilon m/2}=\delta$ เราจะได้ค่าของ $\epsilon$ ซึ่งทำให้สามารถสรุปเป็น generalization bound ได้ว่า

$$
R(h)\leq\frac{2}{m}\left(d\log_2\frac{2em}{d}+\log_2\frac{2}{\delta}\right)
$$

## inconsistent hypothesis
สำหรับกรณีที่ hypothesis $h$ ที่เราเลือกตอบไม่ได้ consistent กับตัวอย่างข้อมูลทั้งหมด เราสามารถใช้ Hoeffding's inequality และเทคนิคเช่นเดียวกันในการจำกัดความน่าจะเป็นที่จะมี $h\in H$ บางตัวที่
$|R(h)-\hat{R}(h)|>\epsilon$ ได้ว่า

$$
\Pr[\exists h\in H: |R(h)-\hat{R}(h)|>\epsilon]\leq 4\Pi_H(2m)e^{-m\epsilon^2/8}
$$

ซึ่งจะได้ generalization bound เป็นดังนี้

$$
R(h)\leq \hat{R}(h)+\sqrt{\frac{8}{m}(d\log\frac{2em}{d}+\log\frac{4}{\delta})}
$$

และหากเราเลือก hypothesis $h_S^\text{ERM}$ ตาม empirical risk minimization เมื่อพิจารณาจากตัวอย่างข้อมูลใน $S$ ถ้า 
$$h^*$$ เป็น hypothesisใน $H$ ที่มี $$R(h^*)$$ น้อยที่สุด เราจะได้ว่า

$$
R(h_S^\text{ERM})\leq R(h^*)+\sqrt{\frac{32}{m}(d\log\frac{2em}{d}+\log\frac{4}{\delta})}
$$
