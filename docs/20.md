{% include lib/mathjax.html %}
# Generalization bound บน VC dimension
หลังจากที่เราได้ทำการวิเคราะห์ sample complexity และ generalization bound ของ hypothesis space ที่มีขนาดจำกัดมาแล้ว
ในหัวข้อนี้เราจะมาลองวิเคราะห์สำหรับ hypothesis space ใด ๆ ดูบ้าง สังเกตว่าเราไม่สามารถใช้ขนาดของ hypothesis space $H$
เป็นตัวแทนของความซับซ้อนของ $H$ ในการวิเคราะห์ได้อีก เนื่องจาก $H$ อาจมีขนาดไม่จำกัดซึ่งจะทำให้เราคำนวณด้วยวิธีเดิมไม่ได้
อย่างไรก็ดี เราสามารถใช้ VC dimension ของ $H$ เป็นตัวแทนของความซับซ้อนของ $H$ และวิเคราะห์ generalization bound ให้อยู่ในรูปของ
VC dimension ได้

## consistent hypothesis
ให้ $H$ เป็น hypothesis space ใด ๆ (ซึ่งอาจมีขนาดจำกัดหรือไม่ก็ได้) ถ้าให้ concept เป้าหมายเป็น $c\in H$ และ $$S=\{(x_1,y_1),\dots,(x_m,y_m)\}$$ เป็นเซตของตัวอย่างข้อมูล $m$ ตัว
หากอัลกอริทึมการเรียนรู้ของเราทำการเลือก hypothesis $h\in H$ ที่ consistent กับ $S$ ได้ เราจะวิเคราะห์โอกาสที่ $h$ ที่ได้มาจะมีความผิดพลาดสูงกว่า $\epsilon$ โดยการหาความน่าจะเป็นของเหตุการณ์ที่มี hypothesis $h\in H$ อย่างน้อยหนึ่งตัวที่ $R(h)>\epsilon$ และ $h$ consistent กับ $S$
ถ้าให้เหตุการณ์ดังกล่าวแทนด้วย $A$ เป้าหมายของเราคือการหาขอบเขตบนของ

$$
\Pr[A] = \Pr[\exists h\in H: R(h)>\epsilon \text{ และ } h \text{ consistent กับ } S]
$$

สำหรับ hypothesis $h$ ที่ $R(h)>\epsilon$ หากเรากำหนดให้ $r$ เป็นพื้นที่ใน $X$ ที่ $h(x)\neq c(x)$ แสดงว่า $h$ จะ consistent กับ $S$ ก็ต่อเมื่อไม่มีข้อมูลตัวอย่าง $x_i$ ตัวใดตกอยู่ใน $r$ เลย สมมติว่าเราสุ่มหยิบตัวอย่างข้อมูลใน $X$ มาอีก $m$ ตัวจากการกระจายตัวแบบเดียวกัน ให้ $$S'=\{(x'_1,y'_1),\dots,(x'_m,y'_m)\}$$ เป็นเซตของตัวอย่างข้อมูลที่หยิบได้ เมื่อพิจารณา Chebyshev's inequality ซึ่งกล่าวไว้ดังนี้

#### Chebyshev's inequality
ให้ $X$ เป็นตัวแปรสุ่มที่มี standard deviation $\sigma_X$ และให้ $t$ เป็นจำนวนจริงใด ๆ ที่ $t>0$ 

$$
\Pr[|X-\mathbb{E}[X]|\geq t\sigma_X]\leq\frac{1}{t^2}
$$

ถ้าเราให้ $X$ เป็นตัวแปรสุ่มแทนจำนวนตัวอย่างข้อมูลใน $S'$ ที่ตกอยู่ในพื้นที่ $r$ เนื่องจากความน่าจะเป็นที่ตัวอย่างข้อมูลจะตกใน $r$ มีอย่างน้อย $\epsilon$ ดังนั้นเราจึงได้ว่า $\mathbb{E}[X]\geq\epsilon m$ ดังนั้น

$$
\begin{split}
\Pr[X\leq\frac{\epsilon m}{2}]&\leq\Pr[|X-\epsilon m|\geq\frac{\epsilon m}{2}]\\
&\leq \frac{4\sigma_X^2}{\epsilon^2m^2}
\end{split}
$$

เนื่องจาก $X$ มีการกระจายตัวเป็น binomial distribution ถ้าให้ $p_r$ เป็นความน่าจะเป็นที่ตัวอย่างข้อมูลจะตกในพื้นที่ $r$ เราจะได้ว่า 

$$
\sigma_X^2 = mp_r(1-p_r)
$$

เนื่องจาก $p(1-p)\leq 1/4$ สำหรับค่า $p$ ใด ๆ ที่ $0\leq p\leq 1$ เราจึงได้ว่า

$$
\Pr[X\leq\frac{\epsilon m}{2}]\leq\frac{m}{\epsilon^2m^2}=\frac{1}{\epsilon^2 m}
$$

ถ้าเรากำหนดให้ $m\geq 2/\epsilon^2$ ก็จะได้

$$
\Pr[X\leq\frac{\epsilon m}{2}]\leq\frac{1}{2}
$$

นั่นคือ เราสรุปได้ว่า หาก $m\geq 2/\epsilon^2$ ตัวอย่างข้อมูลใน $S'$ จะตกในพื้นที่ $r$ อย่างน้อย $\epsilon m/2$ ตัวด้วยความน่าจะเป็นไม่น้อยกว่า 1/2

ถ้าให้ $B$ แทนเหตุการณ์ที่มี hypothesis $h\in H$ ที่ $R(h)>\epsilon$ และ consistent กับ $S$ แต่มีตัวอย่างข้อมูลอย่างน้อย $\epsilon m/2$ ตัวใน $S'$ ที่ $h$ ให้ผลลัพธ์ไม่ถูกต้อง หากเราให้พื้นที่ที่ $h$ ตอบผิดเป็น $r$ เราอาจมองเหตุการณ์ $B$ เป็นเหตุการณ์ที่มีพื้นที่ $r$ ที่ขนาดไม่น้อยกว่า $\epsilon$ ที่ตัวอย่างข้อมูลใน $S$ ไม่ตกลงใน $r$ เลย แต่ตัวอย่างข้อมูลใน $S'$ ตกใน $r$ อย่างน้อย $\epsilon m/2$ ตัว
สังเกตว่า $\Pr[B|A]\geq 1/2$ และเนื่องจาก $B=B\cap A$ เราจึงได้ว่า

$$
\Pr[B]=\Pr[B\cap A]=\Pr[B|A]\Pr[A]\geq\frac{1}{2}\Pr[A]
$$

หรือได้ว่า $\Pr[A]\leq 2\Pr[B]$
ซึ่งทำให้เห็นว่า เราสามารถจำกัดขอบเขตของโอกาสที่เหตุการณ์ $A$ จะเกิดได้โดยการจำกัดโอกาสของเหตุการณ์ $B$ แทน
