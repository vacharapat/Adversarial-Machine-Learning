{% include lib/mathjax.html %}
# การสร้าง adversarial example

จากการนิยาม loss function จะเห็นว่าแบบจำลอง deep learning  $$h_\theta$$ จะมีประสิทธิภาพในการทำนายผลของ input $$x$$ ได้ดีถ้า loss $$\ell(h_\theta(x), y)$$ มีค่าน้อย เมื่อ $$y$$ เป็น label ของคลาสที่ถูกต้องของ $$x$$ ดังนั้น หากเราต้องการก่อกวนแบบจำลองเพื่อให้มีประสิทธิภาพในการทำนายต่ำ เราก็สามารถทำได้โดยการปรับปรุง input $$x$$ เพื่อให้ loss มีค่ามากที่สุด นั่นคือ เราต้องการหา input $$\hat{x}$$ ที่เป็นคำตอบของ

$$
\max_{\hat{x}}\ell(h_\theta(\hat{x}), y)
$$

อย่างไรก็ดี ในความเป็นจริงนั้นใช่ว่าเราจะเลือก $$\hat{x}$$ ใด ๆ มาพิจารณาก็ได้ ตัวอย่างเช่นในปัญหาการจำแนกรูปภาพ หาก $$x$$ เป็นรูปหมูและ $$y$$ เป็น label ของคลาสที่ตอบว่าเป็นหมู หากเราเลือก $$\hat{x}$$ ที่แตกต่างจาก $$x$$ อย่างสิ้นเชิง เช่นเลือก $$\hat{x}$$ เป็นรูปรถ ก็จะไม่สอดคล้องกับความต้องการของเราที่ต้องการ _หลอก_ แบบจำลองให้ตัดสินใจว่ารูปที่รับมาไม่ใช่รูปหมูทั้ง ๆ ที่ในความเป็นจริงรูปนั้นยังคงเป็นรูปหมู ดังนั้นเราจะมองปัญหานี้ใหม่เป็นการใส่ _การก่อกวน_ เล็ก ๆ เข้าไปยัง input $$x$$ เพื่อให้ค่าของ loss สูงที่สุด หากเราแทนการก่อกวนดังกล่าวด้วย $$\delta$$ จะได้ว่าปัญหาที่เราสนใจคือการหา $$\delta$$ ที่เป็นคำตอบของ

$$
\max_{\delta\in\Delta}\ell(h_\theta(x+\delta),y)
$$

โดย $$\Delta$$ เป็นเซตของการก่อกวนที่เป็นไปได้ ในทางอุดมคติเราต้องการให้ $$\Delta$$ เป็นเซตของการก่อกวนที่ยังทำให้มนุษย์เห็นว่าตัวอย่างข้อมูล $$x+\delta$$ นั้นเหมือนกับ $$x$$ ตัวเดิม ในกรณีของรูปภาพ ตัวอย่างการก่อกวนดังกล่าวได้แก่การเพิ่ม noise เล็กน้อยเข้าไปในภาพ การหมุนภาพ การเลื่อนภาพ หรือการเปลี่ยนแปลงส่วนในภาพที่ไม่เกี่ยวข้องกับวัตถุหลัก (เช่นการเปลี่ยนแปลงค่าในพิกเซลที่เป็นท้องฟ้าในรูปหมู) ถึงแม้ว่าเราจะไม่สามารถนิยามเซตของการก่อกวนที่ต้องการทั้งหมดนี้ให้ชัดเจนทางคณิตศาสตร์ได้ การก่อกวนอย่างง่าย ๆ บางรูปแบบก็เพียงพอที่จะทำให้เห็นความเปราะบางของแบบจำลองของเราได้

การก่อกวนที่ถูกนำมาวิเคราะห์เป็นหลักคือการก่อกวนที่อยู่ในรูป

$$
\Delta = \{\delta:\|\delta\|\leq \epsilon\}
$$

เมื่อ $$\|\delta\|$$ แทน norm หรือขนาดของ $$\delta$$ ซึ่ง norm รูปแบบหนึ่งที่น่าสนใจคือ $$\ell_\infty$$-norm ซึ่งมีนิยามดังนี้

$$
\|d\|_\infty = \max_i |d_i|
$$

กล่าวได้อีกอย่างคือ เมื่อเราพิจารณาการก่อกวนในเซต $$\Delta = \{\delta:\|\delta\|_\infty\leq\epsilon\}$$ หมายความว่าเรายอมให้ค่าในแต่ละมิติของ input $$x$$ เพิ่มขึ้นหรือลดลงจากเดิมได้ไม่เกิน $$\epsilon$$ นั่นเอง หากเรากำหนดค่า $$\epsilon$$ น้อย ๆ จนรับประกันว่าภาพที่ถูกก่อกวนแล้วยังคงเห็นได้ไม่ต่างจากเดิมแน่นอน เราก็สามารถใช้การก่อกวนลักษณะนี้ทดลองโจมตีแบบจำลองเพื่อทดสอบความทนทานได้

เมื่อเรานิยามการหา adversarial example เป็นปัญหา optimization ได้ชัดเจนแล้ว เราสามารถแก้ปัญหานี้โดยใช้อัลกอริทึม gradient descent ได้เช่นกัน โดยทำการคำนวณหาเกรเดียนต์ของ loss เมื่อเทียบกับ $$\delta$$ และค่อย ๆ ปรับ $$\delta$$ เพื่อให้ loss เพิ่มขึ้นตามต้องการ โดยเมื่อใดก็ตามที่ $$\delta$$ พ้นขอบเขตที่กำหนด ($$\epsilon$$) เราทำการ project กลับมาให้อยู่ในขอบเขตของ $$\ell_\infty$$ โดยการ _clip_ ค่าที่เกิน $$\epsilon$$ ให้อยู่ที่ $$\epsilon$$ และ clip ค่าที่ต่ำกว่า $$-\epsilon$$ ให้อยู่ที่ $$-\epsilon$$ เราเรียกวิธีการนี้ว่า _projected gradient descent_ (PGD) นอกจากนี้ หากค่าของ input ในมิติใดมีขอบเขตจำกัด (เช่น $$x_i\in [0,1]$$) เราต้องระวังอย่าให้การก่อกวนนี้พาเราออกไปนอกขอบเขตด้วย (เราสามารถป้องกันเหตุการณ์ดังกล่าวโดยใช้การ clip เช่นเดียวกัน) 

ใน [Adversarial Robustness - Theory and Practice](https://adversarial-ml-tutorial.org) มีตัวอย่างการทดสอบการก่อกวนแบบจำลอง deep learning โดยใช้แบบจำลอง ResNet50 ที่ถูกเทรนมาสำหรับจำแนกรูปภาพออกเป็น 1000 คลาส เมื่อเราสั่งให้ทำนายรูปหมูด้านล่างนี้ แบบจำลองสามารถทำนายว่าเป็นรูปหมูได้ด้วยความน่าจะเป็น 0.996 

<p align="center">
<img width="350" src="https://raw.githubusercontent.com/vacharapat/Adversarial-Machine-Learning/master/images/output_0.png">
</p>

เมื่อเราหาการก่อกวน $$\delta$$ ที่ดีที่สุดโดยกำหนด $$\epsilon = 2/255$$ และทำ projected gradient descent ด้วย learning rate 0.1 เป็นจำนวน 30 รอบ เราได้รูปที่ถูกก่อกวนเป็นดังนี้ 

<p align="center">
<img width="350" src="https://raw.githubusercontent.com/vacharapat/Adversarial-Machine-Learning/master/images/output_1.png">
</p>

ซึ่งสังเกตว่าในสายตาของมนุษย์เรานั้นไม่แตกต่างจากเดิมเลย อย่างไรก็ดี ResNet50 คำนวณความน่าจะเป็นที่รูปนี้จะเป็นรูปหมูเหลือน้อยกว่า $$10^{-5}$$ และทำนายว่าเป็นรูปวอมแบตด้วยความน่าจะเป็น 0.9998

เมื่อเรานำ $$\delta$$ นี้มาเพิ่มความเข้มขึ้น 50 เท่า จะได้เป็นรูปด้านล่าง

<p align="center">
<img width="350" src="https://raw.githubusercontent.com/vacharapat/Adversarial-Machine-Learning/master/images/output_2.png">
</p>

จากตัวอย่างนี้จะเห็นว่า เมื่อ input ที่เราจะให้แบบจำลองทำการตัดสินใจถูกปนเปื้อนด้วย noise ที่ดูเหมือนเกิดจากการสุ่ม แม้ว่าการเปลี่ยนแปลงของรูปนั้นน้อยมากจนสายตามนุษย์ไม่สามารถแยกออกได้ ก็ยังสามารถทำให้แบบจำลองทาง machine learning นั้นให้ผลผิดจากเดิมไปอย่างมาก ซึ่งแสดงถึง _ความเปราะบาง_ ของแบบจำลอง ซึ่งในบางสถานการณ์อาจนำมาสู่ผลร้ายได้
