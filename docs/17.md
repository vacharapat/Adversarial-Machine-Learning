{% include lib/mathjax.html %}
# การเรียนรู้เมื่อ hypothesis space มีขนาดจำกัด

ก่อนหน้านี้เราได้ดูตัวอย่างการวิเคราะห์ sample complexity และ generalization bound ของปัญหาหนึ่งมาแล้ว ในหัวข้อนี้เราจะมาวิเคราะห์ในกรณีของปัญหาการเรียนรู้ใด ๆ ที่อัลกอริทึมการเรียนรู้ของเราสามารถสร้าง hypothesis ที่เป็นคำตอบได้เป็นจำนวนจำกัด 

## consistent hypothesis
ถ้าให้ $H$ เป็นเซตของ hypothesis ทั้งหมดที่สร้างได้จากอัลกอริทึมการเรียนรู้ โดยที่ขนาดของ $H$ เป็นจำนวนจำกัด สมมติว่าเราทราบว่า concept เป้าหมาย $c$ ของเรานั้นเป็นสมาชิกใน $H$ ด้วย แนวทางการเรียนรู้หนึ่งที่ทำได้คือ เมื่อได้รับตัวอย่างข้อมูล $S=\{(x_1,y_1),\dots,(x_m,y_m)\}$ มาแล้ว เราเลือกตอบ hypothesis $h_S\in H$ ที่ _สอดคล้อง_ (consistent) กับตัวอย่างข้อมูลทั้งหมด นั่นคือเราเลือกตอบ hypothesis $h_S$ ที่ $h_S(x)=y$ สำหรับตัวอย่างข้อมูล $(x,y\in S$ ทุกตัว

เราจะมาวิเคราะห์กันว่า หากเราเลือกตอบ hypothesis ตามแนวทางนี้ จะสามารถหา sample complexity และ generalization bound ตามแนวทางของ PAC learning ได้อย่างไร

สังเกตว่าถ้าเรากำหนดค่า error parameter $\epsilon>0$ และ confident parameter $\delta>0$ สำหรับ hypothesis $h\in H$ ใด ๆ ที่มี $R(h)>\epsilon$ หรือ $\Pr_{x\sim D}[h(x)\neq c(x)] >\epsilon$ จะเห็นว่าการที่ $h$ จะ consistent กับตัวอย่างข้อมูล $(x,y)$ ใด ๆ แสดงว่าเราต้องสุ่มหยิบ $(x,y)$ มาได้ $h(x)=c(x)$ ซึ่งจะเกิดขึ้นได้ด้วยความน่าจะเป็นไม่เกิน $1-\epsilon$ ดังนั้น ความน่าจะเป็นที่ $h$ จะ consistent กับข้อมูลใน $S$ ทั้ง $m$ ตัวจะต้องมีค่าไม่เกิน $(1-\epsilon)^m$

การที่เรารับตัวอย่างข้อมูล $S$ มาและหา hypothesis ที่ consistent กับ $S$ ทั้งหมดเป็นคำตอบนั้น เราอาจจะได้ hypothesis $h\in H$ ที่ $R(h)>\epsilon$ เป็นคำตอบได้ หากมี hypothesis ดังกล่าวบางตัว consistent กับ $S$ ซึ่งจะเห็นว่าความน่าจะเป็นของเหตุการณ์ดังกล่าวสามารถจำกัดได้ดังนี้

$$
\begin{split}
\Pr[\exists h\in H: R(h)>\epsilon, h \text{ consistent กับ } S]&=\Pr[\bigvee_{h\in H, R(h)>\epsilon} h \text{ consistent กับ } S]\\
&\leq \sum_{h\in H, R(h)>\epsilon}\Pr[h \text{ consistent กับ } S]\\
&\leq\sum_{h\in H, R(h)>\epsilon}(1-\epsilon)^m\\
&\leq |H|(1-\epsilon)^m\\
&\leq |H|e^{-m\epsilon}
\end{split}
$$
