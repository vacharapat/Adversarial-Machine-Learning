{% include lib/mathjax.html %}
# การเรียนรู้เมื่อ hypothesis space มีขนาดจำกัด

ก่อนหน้านี้เราได้ดูตัวอย่างการวิเคราะห์ sample complexity และ generalization bound ของปัญหาหนึ่งมาแล้ว ในหัวข้อนี้เราจะมาวิเคราะห์ในกรณีของปัญหาการเรียนรู้ใด ๆ ที่อัลกอริทึมการเรียนรู้ของเราสามารถสร้าง hypothesis ที่เป็นคำตอบได้เป็นจำนวนจำกัด 

## consistent hypothesis
ถ้าให้ $H$ เป็นเซตของ hypothesis ทั้งหมดที่สร้างได้จากอัลกอริทึมการเรียนรู้ โดยที่ขนาดของ $H$ เป็นจำนวนจำกัด สมมติว่าเราทราบว่า concept เป้าหมาย $c$ ของเรานั้นเป็นสมาชิกใน $H$ ด้วย แนวทางการเรียนรู้หนึ่งที่ทำได้คือ เมื่อได้รับตัวอย่างข้อมูล $$S=\{(x_1,y_1),\dots,(x_m,y_m)\}$$ มาแล้ว เราเลือกตอบ hypothesis $h_S\in H$ ที่ _สอดคล้อง_ (consistent) กับตัวอย่างข้อมูลทั้งหมด นั่นคือเราเลือกตอบ hypothesis $h_S$ ที่ $h_S(x)=y$ สำหรับตัวอย่างข้อมูล $(x,y)\in S$ ทุกตัว

เราจะมาวิเคราะห์กันว่า หากเราเลือกตอบ hypothesis ตามแนวทางนี้ จะสามารถหา sample complexity และ generalization bound ตามแนวทางของ PAC learning ได้อย่างไร

สังเกตว่าถ้าเรากำหนดค่า error parameter $\epsilon>0$ และ confident parameter $\delta>0$ สำหรับ hypothesis $h\in H$ ใด ๆ ที่มี $R(h)>\epsilon$ หรือ $\Pr_{x\sim D}[h(x)\neq c(x)] >\epsilon$ จะเห็นว่าการที่ $h$ จะ consistent กับตัวอย่างข้อมูล $(x,y)$ ใด ๆ แสดงว่าเราต้องสุ่มหยิบ $(x,y)$ มาได้ $h(x)=c(x)$ ซึ่งจะเกิดขึ้นได้ด้วยความน่าจะเป็นไม่เกิน $1-\epsilon$ ดังนั้น ความน่าจะเป็นที่ $h$ จะ consistent กับข้อมูลใน $S$ ทั้ง $m$ ตัวจะต้องมีค่าไม่เกิน $(1-\epsilon)^m$

การที่เรารับตัวอย่างข้อมูล $S$ มาและหา hypothesis ที่ consistent กับ $S$ ทั้งหมดเป็นคำตอบนั้น เราอาจจะได้ hypothesis $h\in H$ ที่ $R(h)>\epsilon$ เป็นคำตอบได้ หากมี hypothesis ดังกล่าวบางตัว consistent กับ $S$ ซึ่งจะเห็นว่าความน่าจะเป็นของเหตุการณ์ดังกล่าวสามารถจำกัดขอบเขตได้ดังนี้

$$
\begin{split}
\Pr[\exists h\in H: R(h)>\epsilon, h \text{ consistent กับ } S]&=\Pr[\bigvee_{h\in H, R(h)>\epsilon} h \text{ consistent กับ } S]\\
&\leq \sum_{h\in H, R(h)>\epsilon}\Pr[h \text{ consistent กับ } S]\\
&\leq\sum_{h\in H, R(h)>\epsilon}(1-\epsilon)^m\\
&\leq |H|(1-\epsilon)^m\\
&\leq |H|e^{-m\epsilon}
\end{split}
$$

ดังนั้นถ้าเราต้องการให้ hypothesis $h_S$ ที่เราเลือกเป็น hypothesis ที่มี error $R(h_S)\leq\epsilon$ ด้วยความน่าจะเป็นไม่ต่ำกว่า $1-\delta$
เราสามารถกำหนดให้ความน่าจะเป็นที่จะมี hypothesis ที่ error มากกว่า $\epsilon$ หลุดรอดมาเป็นตัวเลือกในคำตอบได้มีค่าไม่เกิน $\delta$ ซึ่งทำได้โดยกำหนดให้

$$
|H|e^{-m\epsilon}\leq\delta
$$

ซึ่งจะได้ sample conplexity เป็น

$$
m\geq\frac{1}{\epsilon}(\log|H| + \log\frac{1}{\delta})
$$

หรือเราสามารถวิเคราะห์ generalization bound ได้ว่า ด้วยความน่าจะเป็นอย่างน้อย $1-\delta$ เราจะได้ผลลัพธ์เป็น hypothesis $h_S$ ที่

$$
R(h_S)\leq \frac{1}{m}(\log|H|+\log\frac{1}{\delta})
$$

## การเรียนรู้ conjunction

พิจารณาตัวอย่าง concept class $C$ ที่เป็นเซตของ conjunction ที่สร้างจาก literal ของตัวแปร boolean $x_1,\dots,x_n$ โดย literal อาจเป็น $x_i$ หรือ $\bar{x}_i$ โดยสำหรับการกำหนดค่าความจริงให้ตัวแปรทั้ง $n$ ตัวแบบใดที่ทำให้ conjunction นี้เป็นจริงจะมี label เป็น 1 ในขณะที่การกำหนดค่าแบบอื่น ๆ มี label เป็น 0 ตัวอย่างเช่น ถ้า $n=4$ และ concept เป้าหมายเป็น conjunction $x_1\land\bar{x}_3\land x_4$ เราจะได้ว่า $(1,0,0,1)$ เป็นตัวอย่างข้อมูลที่มี label เป็น 1 และ $(1,1,1,0)$ มี label เป็น 0

สังเกตว่าตัวอย่างข้อมูลที่มี label เป็น 1 เช่น $(1,0,0,1)$ นั้นบอกเราว่าใน conjunction เป้าหมายจะต้องมี $\bar{x}_1,x_2,x_3,$ หรือ $\bar{x}_4$ ไม่ได้ ในขณะที่ตัวอย่างข้อมูลที่มี label เป็น 0 นั้นไม่สามารถทำให้เราสรุปอะไรได้
จากตรงนี้วิธีหนึ่งที่จะหา conjunction ที่สอดคล้องกับข้อมูลตัวอย่างทั้งหมดทำได้โดยเริ่มต้นสมมติให้ hypothesis $h$ ของเราเป็น conjunction ที่ประกอบด้วย literal ทั้งหมดที่เป็นไปได้ นั่นคือ 

$$
h=x_1\land\bar{x}_1\land\dots\land x_n\land\bar{x}_n
$$

จากนั้นเมื่อเราพิจารณาตัวอย่างข้อมูลแต่ละตัว หากตัวอย่างข้อมูลของเรามี label เป็น 1 และ hypothesis $h$ ของเรายังคงมี literal ที่ขัดแย้งกับตัวอย่างข้อมูลนั้น เราก็จะลบ literal ดังกล่าวออกจาก $h$ เมื่อเราทำเช่นนี้ไปเรื่อย ๆ จนเราพิจารณาตัวอย่างข้อมูลครบทุกตัว ก็จะได้ $h$ เป็น conjunction ที่สอดคล้องกับตัวอย่างข้อมูลทั้งหมดแน่นอน

เนื่องจากใน conjunction ใด ๆ สำหรับตัวแปร $x_i$ แต่ละตัว เรามีทางเลือกในการปรากฏอยู่ใน conjunction ทั้งสิ้น 3 ทางเลือก คือจะปรากฏเป็น $x_i$ หรือ $\bar{x}_i$ หรือไม่ปรากฏเลยก็ได้ ดังนั้นจะเห็นว่าจำนวน conjunction ทั้งหมดที่เป็นไปได้จะมีจำนวนเท่ากับ $3^n$ 

จากการวิเคราะห์ของเราเมื่อ hypothesis space มีจำนวนจำกัด เราจึงได้ว่าอัลกอริทึมการเรียนรู้ conjunction นี้สามารถเรียนรู้ได้โดยมี sample complexity เป็น $\frac{1}{\epsilon}(n\log 3+\log\frac{1}{\delta})$ หรือมี generalization bound เป็น

$$
R(h)\leq \frac{1}{m}(n\log 3+\log\frac{1}{\delta})
$$
