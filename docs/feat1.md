{% include lib/mathjax.html %}
# คุณสมบัติของ adversarial example

ในหัวข้อก่อนหน้านี้ เมื่อเราได้ค้นพบจุดอ่อนของแบบจำลอง machine learning ทั่วไปและสามารถสร้าง
adversarial example ได้ เราก็ได้หาวิธีการเพิ่มความทนทานของแบบจำลองต่อ adversarial example เหล่านี้ รวมไปถึงเทคนิคในการตรวจสอบความทนทานของแบบจำลองกันด้วย อย่างไรก็ดี สิ่งที่เราดูกันมานั้นยังไม่ได้อธิบายถึงสาเหตุความเปราะบางของแบบจำลอง และมีอยู่ของ adversarial example
เหล่านี้ว่าเกิดขึ้นได้อย่างไร

ในหัวข้อนี้เราจะมาทำความเข้าใจเกี่ยวกับสาเหตุของความเปราะบางของแบบจำลองและการเกิด adversarial example  โดยเริ่มจากการศึกษาคุณสมบัติของ
adversarial example ที่สร้างขึ้นมาได้

## การโจมตีข้ามแบบจำลอง
คุณสมบัติแรกของ adversarial example ที่น่าสนใจก็คือ adversarial example จำนวนมากมีความสามารถในการโจมตี
_ข้าม_ แบบจำลองได้ กล่าวคือ ถ้าให้ $M_A$ และ $M_B$ เป็นแบบจำลองทาง machine learning
ที่เป็นอิสระจากกัน (ไม่จำเป็นต้องมีโครงสร้างแบบจำลองเหมือนกัน) เมื่อเราเทรนแบบจำลอง $M_A$ และ $M_B$
ด้วย training data ที่มีจากการกระจายตัวของข้อมูลแบบเดียวกัน (ไม่จำเป็นต้องเป็น training data ชุดเดียวกัน)
จนแบบจำลองทั้งคู่มีความแม่นยำสูง เราจะพบว่า ถ้าให้ $x'$ เป็น adversarial example ที่เราสร้างขึ้นเพื่อโจมตี
$M_A$ นั่นคือ ทำให้ $M_A$ ทำนายคลาสของ $x'$ ผิดพลาด เมื่อเรานำ $x'$ นี้ไปทดลองให้ $M_B$ ตัดสินใจ
กลับพบว่า $M_B$ ก็ตัดสินใจผิดพลาดด้วยเช่นเดียวกัน นอกจากนี้เมื่อแบบจำลองที่แตกต่างกันทำนาย adversarial example ตัวหนึ่งผิดพลาด ก็มักจะทำนายผิดไปเป็นคลาสเดียวกันอีกด้วย

คุณสมบัตินี้สร้างความสงสัยให้แก่นักวิจัยเป็นอย่างมาก เนื่องจากในการสร้าง adversarial example $x'$
เพื่อโจมตี $M_A$ นั้น เราใช้โครงสร้างของแบบจำลอง $M_A$ เป็นข้อมูลในการคำนวณด้วย แต่ผลลัพธ์ที่ได้กลับสามารถโจมตีแบบจำลองอื่นได้ด้วยเช่นกัน ทั้ง ๆ ที่ไม่จำเป็นต้องมีโครงสร้างแบบจำลองหรือพารามิเตอร์ต่าง ๆ เหมือนกันทั้งหมดก็ได้

นอกจากสร้างปัญหาในมุมมองของนักวิจัยแล้ว คุณสมบัตินี้ก็ยังมีผลกระทบอย่างรุนแรงในระบบต่าง ๆ ที่มีการใช้ machine learning ในส่วนที่สำคัญ เช่นการระบุตัวตนด้วยใบหน้าบน iPhone หรือการตรวจจับวัตถุต่าง ๆ ของรถยนต์ขับเคลื่อนอัตโนมัติ เนื่องจากต่อให้เราไม่รู้ว่าระบบเหล่านี้ใช้โครงสร้างแบบจำลองใดในการตัดสินใจ
แต่ตัวอย่างข้อมูลที่เป็น adversarial example นั้นโจมตีข้ามแบบจำลองได้ นั่นแสดงว่า ถ้ามีผู้ไม่หวังดีสร้าง adversarial example โดยอ้างอิงจากแบบจำลองหนึ่ง และนำไปโจมตีระบบที่เกี่ยวข้องกับความปลอดภัยเหล่านี้
ก็อาจจะสามารถโจมตีได้จริง

## ทิศทางของการก่อกวน และการกระจุกตัวของข้อมูล
สำหรับตัวอย่างข้อมูล $(x,y)$ ใด ๆ เมื่อเรานำมาคำนวณหา adversarial example $x'$
เราจะเรียกทิศทางของเวกเตอร์ที่ลากจาก $x$ ไปยัง $x'$ ว่า _ทิศทางของการก่อกวน_
เช่นรูปด้านล่างนี้แสดงตัวอย่างการจำแนกรูปตัวเลข โดยรูปเลข 9 ในตำแหน่งกลางกล่องสี่เหลี่ยมแสดงตัวอย่างข้อมูลในชุดข้อมูลจริง และรูปเลข 9 ที่ขอบด้านขวาแสดงตัวอย่าง
adversarial example ที่ทำให้แบบจำลองทำนายเป็นเลข 4
โดยกล่องสี่เหลี่ยมแทนขอบเขตของข้อมูลที่มีระยะห่างจากข้อมูลตั้งต้นไม่เกิน $\epsilon$
ทิศทางของการก่อกวนในตัวอย่างนี้แสดงด้วยลูกศรสีเขียว

<p align="center">
<img width="500" src="https://raw.githubusercontent.com/vacharapat/Adversarial-Machine-Learning/master/images/adv_ex.png">
</p>

## References

1. [I. Goodfellow, J. Shlens, C. Szegedy. Explaining and Harnessing Adversarial Examples,
In Intenational Conference on Learning Representations (ICLR), 2015](https://arxiv.org/abs/1412.6572)

---
Prev: [Semidefinite programming relaxation](https://vacharapat.github.io/Adversarial-Machine-Learning/docs/cert4)

Next:
