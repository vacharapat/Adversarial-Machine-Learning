{% include lib/mathjax.html %}
# แบบจำลอง deep learning พอสังเขป

พิจารณาปัญหาการจำแนกข้อมูล ถ้าให้ $$\mathcal{X}$$ เป็น input space เราสามารถมองแบบจำลอง deep learning เป็นฟังก์ชัน 

$$
h_\theta : \mathcal{X}\rightarrow \mathbb{R}^k
$$ 

ที่รับข้อมูลจาก $$\mathcal{X}$$ และให้ผลเป็นเวกเตอร์ของจำนวนจริง $$k$$ มิติ เมื่อ $$k$$ เป็นจำนวนของชนิดข้อมูลที่ต้องการจำแนก ตัวอย่างเช่น หากเราต้องการจำแนกภาพสีขนาด 244$$\times$$244 พิกเซล ออกเป็น 1000 ชนิด เราสามารถสร้างแบบจำลองให้รับ input $$x$$ ใด ๆ เป็นเวกเตอร์ขนาด 244$$\times$$244$$\times$$3 มิติ โดยที่สมาชิกแสดงความเข้มของแสงสีแดง / เขียว / น้ำเงิน ในแต่ละพิกเซล และให้ผลลัพธ์เป็นเวกเตอร์ 1000 มิติ ($$k=1000$$) สัญลักษณ์ $$\theta$$ ใน $$h_\theta$$ เป็นเวกเตอร์ที่แสดงพารามิเตอร์ทั้งหมดของแบบจำลอง เช่นค่า weight matrix ใน fully-connected layer หรือค่า weight ใน convolutional filter เป็นต้น ซึ่งค่าของเวกเตอร์ $$\theta$$ นี้จะถูกปรับให้เหมาะสมที่สุดเท่าที่จะทำได้ในขั้นตอนของการเทรนข้อมูลนั่นเอง

## Loss function
ในการพิจารณาความเหมาะสมของ $$\theta$$ เราต้องเริ่มจากการนิยาม loss function 

$$
\ell: \mathbb{R}^k\times \mathbb{Z}_+\rightarrow \mathbb{R}_+
$$ 

ให้เป็นฟังก์ชันที่รับผลลัพธ์จาก $$h_\theta$$ กับ label ที่ถูกต้อง และคืนค่าเป็นจำนวนจริงที่ไม่น้อยกว่าศูนย์ ซึ่งเราจะเรียกว่า loss หรือมูลค่าความเสียหายเมื่อเทียบผลการทำนายกับคลาสที่ถูกต้อง กล่าวอีกอย่างคือ 

$$
\ell(h_\theta(x), y)
$$ 

เป็น loss หรือความเสียหายเมื่อคลาสที่ถูกต้องคือ $$y$$ และผลการทำนายเป็น $$h_\theta(x)$$

โดยทั่วไปแล้ว สำหรับการจำแนกข้อมูลหลายคลาสเรามักจะแปลงผลของ $$h_\theta(x)$$ ให้อยู่ในรูปการกระจายความน่าจะเป็นโดยใช้ฟังก์ชัน _softmax_ $$\sigma: \mathbb{R}^k\rightarrow\mathbb{R}^k$$ ซึ่งมีนิยามดังนี้

$$
\sigma(z)_i=\frac{e^{z_i}}{\sum_{i=1}^ke^{z_j}}
$$

สำหรับตัวอย่างข้อมูล $$(x, y)$$ เมื่อ $$x$$ เป็น input และ $$y$$ เป็นคลาสที่ถูกต้องของ $$x$$ 
หลังจากที่เราทำการคำนวณกระจายความน่าจะเป็น $$\sigma(h_\theta(x))$$ แล้ว เราต้องการให้ความน่าจะเป็นของคลาสที่ถูกต้องซึ่งคือค่าของ $$\sigma(h_\theta(x))_y$$ มีค่ามากที่สุดเพื่อที่จะได้ทำนายคลาสผลลัพธ์ได้ถูกต้อง 
โดยปกติความน่าจะเป็นนี้มักจะมีค่าน้อยมาก เราจึงมักจะพิจารณาค่า logarithm ของความน่าจะเป็นนี้แทน นั่นคือ เราต้องการให้

$$
\begin{split}
\log\sigma(h_\theta(x))_y &= \log\left(\frac{e^{h_\theta(x)_y}}{\sum_{j=1}^ke^{h_\theta(x)_j}}\right)\\
&= h_\theta(x)_y - \log\sum_{j=1}^ke^{h_\theta(x)_j}
\end{split}
$$

มีค่ามากที่สุด
จากตรงนี้ จะเห็นว่าหากเรานิยามให้ loss ที่เกิดจากตัวอย่างข้อมูล $$(x, y)$$ เป็น

$$
\ell(h_\theta(x), y) = - \log\sigma(h_\theta(x))_y  = \log\sum_{j=1}^ke^{h_\theta(x)_j} - h_\theta(x)_y
$$ 

เราก็จะได้ว่าแบบจำลองนี้สอดคล้องกับตัวอย่างข้อมูล $$(x, y)$$ เมื่อ loss $$\ell(h_\theta(x), y)$$ มีค่าน้อย ฟังก์ชัน loss นี้มีชื่อเรียกว่า _softmax cross entropy_

## การเทรนแบบจำลอง

ในกระบวนการเทรนแบบจำลอง deep learning นั้น เราจะได้รับเซตของตัวอย่างข้อมูล $$S=\{(x_1, y_1),\dots,(x_m,y_m)\}$$ โดยที่ $$x_i\in \mathcal{X}$$ และ $$y_i\in\{1,\dots,k\}$$ สำหรับ $$i=1,\dots, m$$ เราเรียก $$S$$ นี้ว่า training set ซึ่งจะใช้ในการคำนวณค่าของพารามิเตอร์ $$\theta$$ ที่เหมาะสม โดยเราต้องการหาค่าของ $$\theta$$ ที่ทำให้ค่าเฉลี่ย loss ของตัวอย่างข้อมูลใน $$S$$ มีค่าน้อยที่สุด เราสามารถเขียนปัญหานี้ในรูปแบบของปัญหา optimization ได้ดังนี้

$$
\min_\theta \frac{1}{m}\sum_{i=1}^m\ell(h_\theta(x_i),y_i)
$$

อัลกอริทึมพื้นฐานในการแก้ปัญหานี้คือการวนรอบทำ _stochastic gradient descent_ เพื่อค่อย ๆ ปรับค่า $$\theta$$ ให้ค่าเฉลี่ยของ loss นี้ลดลงเรื่อย ๆ โดยในแต่ละรอบ เราพิจารณา _minibatch_ $$\mathcal{B}\subseteq S$$ และทำการคำนวณ _เกรเดียนต์_ (gradient) ของค่าเฉลี่ย loss ของ $$\mathcal{B}$$ เมื่อเทียบกับ $$\theta$$ ซึ่งเขียนแทนด้วย 

$$
\nabla_\theta\left(\frac{1}{|\mathcal{B}|}\sum_{(x,y)\in\mathcal{B}}\ell(h_\theta(x),y)\right) 
= \frac{1}{|\mathcal{B}|}\sum_{(x,y)\in\mathcal{B}}\nabla_\theta\ell(h_\theta(x),y)
$$
