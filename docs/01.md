{% include lib/mathjax.html %}
# แบบจำลอง deep learning พอสังเขป

พิจารณาปัญหาการจำแนกข้อมูล ถ้าให้ $$\mathcal{X}$$ เป็น input space เราสามารถมองแบบจำลอง deep learning เป็นฟังก์ชัน 

$$
h_\theta : \mathcal{X}\rightarrow \mathbb{R}^k
$$ 

ที่รับข้อมูลจาก $$\mathcal{X}$$ และให้ผลเป็นเวกเตอร์ของจำนวนจริง $$k$$ มิติ เมื่อ $$k$$ เป็นจำนวนของชนิดข้อมูลที่ต้องการจำแนก ตัวอย่างเช่น หากเราต้องการจำแนกภาพสีขนาด 244$$\times$$244 พิกเซล ออกเป็น 1000 ชนิด เราสามารถสร้างแบบจำลองให้รับ input $$x$$ ใด ๆ เป็นเวกเตอร์ขนาด 244$$\times$$244$$\times$$3 มิติ โดยที่สมาชิกแสดงความเข้มของแสงสีแดง / เขียว / น้ำเงิน ในแต่ละพิกเซล และให้ผลลัพธ์เป็นเวกเตอร์ 1000 มิติ ($$k=1000$$) สัญลักษณ์ $$\theta$$ ใน $$h_\theta$$ เป็นเวกเตอร์ที่แสดงพารามิเตอร์ทั้งหมดของแบบจำลอง เช่นค่า weight matrix ใน fully-connected layer หรือค่า weight ใน convolutional filter เป็นต้น ซึ่งค่าของเวกเตอร์ $$\theta$$ นี้จะถูกปรับให้เหมาะสมที่สุดเท่าที่จะทำได้ในขั้นตอนของการเทรนข้อมูลนั่นเอง

ในการพิจารณาความเหมาะสมของ $$\theta$$ เรานิยาม loss function 

$$
\ell: \mathbb{R}^k\times \mathbb{Z}_+\rightarrow \mathbb{R}_+
$$ 

เป็นฟังก์ชันที่รับผลลัพธ์จาก $$h_\theta$$ กับ label ที่ถูกต้อง และคืนค่าเป็นจำนวนจริงที่ไม่น้อยกว่าศูนย์ ซึ่งเราใช้แทน loss หรือมูลค่าความเสียหายเมื่อเทียบผลการทำนายกับคลาสที่ถูกต้อง กล่าวอีกอย่างคือ 

$$
\ell(h_\theta(x), y)
$$ 

เป็น loss หรือความเสียหายเมื่อคลาสที่ถูกต้องคือ $$y$$ และผลการทำนายเป็น $$h_\theta(x)$$

โดยทั่วไปแล้ว สำหรับการจำแนกข้อมูลหลายคลาสเรามักจะแปลงผลของ $$h_\theta(x)$$ ให้อยู่ในรูปการกระจายความน่าจะเป็นโดยใช้ฟังก์ชัน _softmax_ $$\sigma: \mathbb{R}^k\rightarrow\mathbb{R}^k$$ ซึ่งมีนิยามดังนี้

$$
\sigma(z)_i=\frac{e^{z_i}}{\sum_{i=1}^ke^{z_j}}
$$

สำหรับตัวอย่างข้อมูล $$(x, y)$$ เมื่อ $$x$$ เป็น input และ $$y$$ เป็นคลาสที่ถูกต้องของ $$x$$ 
หลังจากที่เราทำการคำนวณกระจายความน่าจะเป็น $$\sigma(h_\theta(x))$$ แล้ว เราต้องการให้ความน่าจะเป็นของคลาสที่ถูกต้องซึ่งคือค่าของ $$\sigma(h_\theta(x))_y$$ มีค่ามากที่สุดเพื่อที่จะได้ทำนายคลาสผลลัพธ์ได้ถูกต้อง 
โดยปกติความน่าจะเป็นนี้มักจะมีค่าน้อยมาก เราจึงมักจะพิจารณาค่า logarithm ของความน่าจะเป็นนี้แทน นั่นคือ เราต้องการให้

$$
\begin{split}
\log\sigma(h_\theta(x))_y &= \log\left(\frac{e^{h_\theta(x)_y}}{\sum_{j=1}^ke^{h_\theta(x)_j}}\right)\\
&= h_\theta(x)_y - \log\sum_{j=1}^ke^{h_\theta(x)_j}
\end{split}
$$

มีค่ามากที่สุด
จากตรงนี้ จะเห็นว่าเราสามารถกำหนดให้ loss ที่เกิดจากตัวอย่างข้อมูล $$(x, y)$$ เป็น

$$
\ell(h_\theta(x), y) = - \log\sigma(h_\theta(x))_y  = \log\sum_{j=1}^ke^{h_\theta(x)_j} - h_\theta(x)_y
$$ 

เราก็จะได้ว่าแบบจำลองนี้สอดคล้องกับตัวอย่างข้อมูล $$(x, y)$$ เมื่อ loss $$\ell(h_\theta(x), y)$$ มีค่าน้อย
