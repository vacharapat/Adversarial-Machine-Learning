{% include lib/mathjax.html %}
# VC dimension
ในหัวข้อนี้เราจะมาทำความรู้จักเครื่องมือสำคัญในการอธิบายความซับซ้อนหรือความสามารถในการจำแนกข้อมูลของ hypothesis space ใด ๆ สองอย่างด้วยกัน ได้แก่ growth function และ VC dimension

สำหรับเซตข้อมูล $S$ ใด ๆ หากมี hypothesis $h\in H$ ที่สอดคล้องกับ $S$ เสมอไม่ว่า label ที่ถูกต้องของ $S$ จะเป็นอย่างไร เราจะกล่าวว่า $H$ _shatter_ $S$ หรือ $S$ ถูก shatter โดย $H$

#### Growth function

สำหรับ hypothesis space $H$ ใด ๆ เรานิยามให้ growth function ของ $H$ คือฟังก์ชัน $\Pi_H:\mathbb{N}\to\mathbb{N}$ ที่ $\Pi_H(m)$ เป็นจำนวนวิธีที่มากที่สุดที่เราจะจำแนกข้อมูลกลุ่มหนึ่งซึ่งมีสมาชิก $m$ ตัวโดยใช้ hypothesis ใน $H$ หรือกล่าวได้ว่า สำหรับ $m\in\mathbb{N}$

$$
\Pi_H(m)=\max_{x_1,\dots,x_m \in X}|\{(h(x_1),\dots,h(x_m)): h\in H\}|
$$

สังเกตว่าค่า $\Pi_H(m)$ ที่เป็นไปได้จะมีค่าตั้งแต่ 1 จนถึง $2^m$ และถ้า $\Pi_H(m)=2^m$ แสดงว่ามีชุดข้อมูล $S$ ที่มีขนาดเท่ากับ $m$ อย่างน้อยชุดหนึ่งที่ถูก shatter ได้ด้วย $H$ นั่นคือเรารับประกันได้ว่าจะต้องมี hypothesis $h\in H$ ที่สอดคล้องกับ $S$ เสมอไม่ว่า label ที่ถูกต้องของข้อมูลแต่ละตัวใน $S$ จะเป็นอย่างไรก็ตาม

#### VC dimension

สำหรับ hypothesis space $H$ ใด ๆ เราจะเรียกจำนวนของตัวอย่างข้อมูล $S$ ที่มีจำนวนมากที่สุดที่ถูก shatter ได้โดย $H$ ว่าเป็น VC dimension ของ $H$ นั่นคือ VC dimension ของ $H$ คือจำนวนเต็ม $m$ ที่มากที่สุดที่ $\Pi_H(m)=2^m$


## ตัวอย่าง VC dimension
การวิเคราะห์ VC dimension นั้นมักจะมีกระบวนการดังนี้ หากเราจะแสดงให้เห็นว่า VC dimention ของ hypothesis space $H$ มีค่าเท่ากับ $d$
เราต้องเริ่มจากแสดงให้เห็นว่า VC dimension มีค่าไม่น้อยกว่า $d$ นั่นคือมีชุดข้อมูล $d$ ตัวที่ถูก shatter ได้โดย $H$ นั่นคือ การ label ตัวอย่างข้อมูลทั้ง $2^d$ แบบจะต้องมี hypothesis ใน $H$ ที่สอดคล้องเสมอ จากนั้นเราต้องแสดงให้เห็นว่า VC dimension มีค่าไม่เกิน $d$ โดยบอกว่า สำหรับชุดข้อมูลใด ๆ ที่มีจำนวนตัวอย่างข้อมูล $d+1$ ตัว จะไม่สามารถถูก shatter โดย $H$ ได้

พิจารณากรณีที่ตัวอย่างข้อมูล $x$ ใด ๆ เป็นจุดใน $\mathbb{R}^2$ และให้ $H$ เป็นเซตของเส้นตรงทั้งหมดใน $\mathbb{R}^2$ สังเกตว่าหากเราพิจารณาจุดสามจุดที่ไม่ได้เรียงต่อกันในแนวเส้นตรง ไม่ว่า label ของแต่ละจุดจะเป็นอย่างไร เราจะสามารถหาเส้นตรงที่แบ่งจุดตามต้องการได้เสมอ จากตรงนี้เราได้ว่า VC dimension ของ $H$ ต้องไม่น้อยกว่า 3

คราวนี้พิจารณาเซตของตัวอย่างข้อมูลจำนวน 4 ตัวใด ๆ เราจะแบ่งการวิเคราะห์ออกเป็นสองกรณีได้แก่ เมื่อ convex hull ของข้อมูลทั้ง 4 ตัวนั้นเป็นรูปสี่เหลี่ยม และกรณีอื่น ๆ
สำหรับกรณีที่ convex hull เป็นรูปสี่เหลี่ยม จะเห็นว่าหาก label ของข้อมูลอยู่ในลักษณะตามรูปด้านล่าง เราจะไม่สามารถหาเส้นตรงบนระนาบที่สามารถแบ่งข้อมูลทั้งสองกลุ่มออกจากกันได้

<p align="center">
<img width="200" src="https://raw.githubusercontent.com/vacharapat/Adversarial-Machine-Learning/master/images/vcd1.png">
</p>

ส่วนในกรณีที่ convex hull ไม่ได้เป็นรูปสี่เหลี่ยม แสดงว่าจะต้องมีจุดบางจุดถูกล้อมรอบด้วยจุดอื่น ซึ่งถ้าเราให้จุดที่ถูกล้อมรอบมี label แตกต่างจากจุดที่ติดกับ convex hull เช่นรูปด้านล่าง ก็จะไม่สามารถหาเส้นตรงบนระนาบที่แบ่งข้อมูลได้สอดคล้องอีกเช่นกัน

<p align="center">
<img width="200" src="https://raw.githubusercontent.com/vacharapat/Adversarial-Machine-Learning/master/images/vcd2.png">
</p>

ดังนั้นเราจึงได้ว่า VC dimension ของ $H$ จะต้องน้อยกว่า 4 แน่นอน และเมื่อเราทราบว่า VC dimension จะต้องไม่น้อยกว่า 3 แต่ต้องน้อยกว่า 4 เราจึงสรุปได้ว่า VC dimension นี้ต้องมีค่าเท่ากับ 3 

## Sauer's lemma
หากเราทราบ VC dimension ของ hypothesis space $H$ เราจะสามารถหาขอบเขตของ growth function ของ $H$ ได้ โดยถ้าให้ $d$ เป็น VC dimension ของ $H$ สำหรับจำนวนเต็ม $m\geq 1$ เราจะได้ว่า

$$
\Pi_H(m)\leq\sum_{i=0}^d{m\choose i}
$$

**พิสูจน์** เราสามารถพิสูจน์ความจริงนี้ได้โดยการทำ induction บน $m$ และ $d$ 
โดยหากพิจารณาสำหรับ $m$ ใด ๆ เมื่อ $d=0$ เราจะได้ว่า 

$$
\Pi_H(m)\leq 1 = {m\choose 0}
$$

แน่นอน และหาก $m=1$ สำหรับ $d\geq 1$
เราจะได้ว่า

$$
\Pi_H(1)=2={1\choose 0}+{1\choose 1}\leq\sum_{i=0}^d{1\choose i}
$$ 

ถูกต้องเช่นเดียวกัน (เรากำหนดให้ ${m\choose i} = 0$ เมื่อ $i>m$)

คราวนี้พิจารณาสำหรับค่าของ $m$ และ $d$ ใด ๆ สมมติให้ $$S=\{x_1,\dots,x_m\}$$ เป็นเซตของตัวอย่างข้อมูล $m$ ตัวที่สามารถแยกประเภทได้ $\Pi_H(m)$ แบบ ถ้าให้ 
$G=H_{|S}$
เป็นเซตของ hypothesis ใน $H$ ทั้งหมดที่จำกัดอยู่บนตัวอย่างข้อมูลใน $S$ เท่านั้น สังเกตว่าขนาดของ $G$ จะต้องเท่ากับ $\Pi_H(m)$

ให้ $$S'=S-\{x_m\}$$ เรากำหนดให้ 
$G_1=G_{|S'}$ เป็นเซตของ hypothesis ใน $G$ ที่จำกัดอยู่บน $S'$ อีกที หากเราพิจารณา hypothesis $h$ ใด ๆ ในรูปของเซต 
$$\{x\in S: h(x)=1\}$$ เราจะนิยาม hypothesis $G_2$ เป็น

$$
G_2=\{g'\subseteq S': g'\in G \text{ และ } (g'\cup\{x_m\})\in G\}
$$

เราจะได้ว่า 
$|G|=|G_1|+|G_2|$

เนื่องจาก $G_1\subseteq G\subseteq H$ ดังนั้น VC dimension ของ $G_1$ จะต้องมีค่าไม่เกิน $d$ ซึ่งทำให้เราได้ว่า

$$
|G_1|\leq\Pi_{G_1}(m-1)\leq\sum_{i=0}^{d}{m-1\choose i}
$$

นอกจากนี้ สังเกตว่าถ้าเซต $Z\subseteq S'$ ถูก shatter โดย $G_2$ เราจะได้ว่า $$Z\cup\{x_m\}$$ ก็จะต้องถูก shatter โดย $G$ ด้วย
นั่นแสดงว่า VC dimension ของ $G_2$ จะต้องมีค่าน้อยกว่า VC dimension ของ $G$ แน่นอน หรือกล่าวได้ว่า VC dimension ของ $G_2$ จะต้องมีค่าไม่เกิน $d-1$ ซึ่งทำให้เราได้ว่า

$$
|G_2|\leq\Pi_{G_2}(m-1)\leq\sum_{i=0}^{d-1}{m-1\choose i}
$$

เพราะฉะนั้น

$$
\begin{split}
|G| &=|G_1|+|G_2|\\
&\leq\sum_{i=0}^d{m-1\choose i} +\sum_{i=0}^{d-1}{m-1\choose i}
\end{split}
$$
