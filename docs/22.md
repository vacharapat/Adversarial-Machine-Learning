{% include lib/mathjax.html %}
# Lower bound

ในหัวข้อก่อนหน้า เราได้เห็นขอบเขตบนของ generalization error หรือ sample complexity ของการเรียนรู้แบบ PAC กันมาแล้ว
ในหัวข้อนี้เราจะมาวิเคราะห์ _ขอบเขตล่าง_ (lower bound) กันดูบ้าง เนื่องจากกรอบการเรียนรู้แบบ PAC นี้ไม่ขึ้นกับการกระจายของข้อมูล กล่าวคือ 
เราสามารถประยุกต์ใช้ผลจากการศึกษาของ PAC ได้โดยไม่จำเป็นต้องทราบว่าการกระจายของข้อมูลที่เราสนใจมีลักษณะเป็นอย่างไร
ขอเพียงเป็นการกระจายที่คงที่แบบใดแบบหนึ่ง ดังนั้นในการหาขอบเขตล่างเราอาจทำได้โดยแสดงให้เห็นการกระจายของข้อมูลที่ generalization error ไม่มีทางต่ำกว่าค่าหนึ่งด้วยความน่าจะเป็นที่มากกว่าศูนย์ หรือในมุมของ sample complexity ก็คือ เราจะแสดงให้เห็นว่ามีการกระจายของข้อมูลที่ เมื่อจำนวนตัวอย่างข้อมูลน้อยกว่าค่าหนึ่ง เรามีโอกาสที่ hypothesis ผลลัพธ์จากการเรียนรู้จะมี risk มากกว่า $\epsilon$ ด้วยความน่าจะเป็นที่มากกว่าศูนย์

## $\Omega(d)$- sample complexity bound
ในขั้นแรกนี้เราจะแสดงตัวอย่างการวิเคราะห์ขอบเขตล่าง โดยจะแสดงให้เห็นว่าจำนวนตัวอย่างข้อมูลที่ต้องการเพื่อเรียนรู้ให้สำเร็จนั้นมีขอบเขตล่างเป็น $\Omega(d)$ เมื่อ $d$ เป็น VC dimenstion ของ hypothesis space ที่เราสนใจ

พิจารณา hypothesis space $H$ ที่มี VC dimension เท่ากับ $d$ ให้ $$X=\{x_1,\dots,x_d\}$$ เป็นเซตของตัวอย่างข้อมูลที่ถูก shatter ได้ด้วย $H$ 
และกำหนดให้ตัวอย่างข้อมูล $x_i$ แต่ละตัวมีความน่าจะเป็นที่จะถูกสุ่มได้เท่ากับ $1/d$ เท่ากันทั้งหมด สังเกตว่าเราสามารถมอง $H$ ให้มีขนาดจำกัดโดยมีจำนวน hypothesis เท่ากับ $2^d$ โดยที่ สำหรับการ label ตัวอย่างข้อมูลใน $X$ แต่ละแบบ จะมี hypothesis ใน $H$ เพียงแบบเดียวเท่านั้นที่สอดคล้องกับการ label ดังกล่าว สังเกตว่าการสุ่มเลือก concept เป้าหมาย $c\in H$ นั้นสามารถทำได้โดยการสุ่มเลือก label ให้กับข้อมูล $x_i\in X$ แต่ละตัว

คราวนี้สมมติให้ concept เป้าหมาย $c$ นั้นถูกสุ่มมาจาก $H$ จากนั้น เมื่อเราสุ่มหยิบตัวอย่างข้อมูลใน $X$ มา $m$ ตัวได้เป็นเซต $$S=\{x_1,\dots,x_{m}\}$$ 
เราจะได้ว่าผลของการดำเนินการนี้จะมีการกระจายเหมือนกับเราสุ่มหยิบตัวอย่างข้อมูล $S$ จาก $X$ มาก่อนจำนวน $m$ ตัวโดยข้อมูลทุกตัวยังไม่ถูก label จากนั้นเราค่อยสุ่ม label $c(x_i)$ ให้กับตัวอย่างข้อมูลแต่ละตัวทั้งที่อยู่ใน $S$ และไม่อยู่ใน $S$ แล้วให้ $c$ เป็น concept ใน $H$ ที่สอดคล้องกับการ label ที่ได้นี้

จากตรงนี้ถ้าเราให้ $h_S\in H$ เป็น hypothesis ที่ consistent กับ $S$ และให้ $R_c(h_S)$ แทน risk ของ $h_S$ เมื่อเทียบกับ concept $c$ จะได้ว่า

$$
\mathbb{E}_c[\mathbb{E}_S[R_c(h_S)]]=mathbb{E}_S[\mathbb{E}_c[R_c(h_S)]]
$$

คราวนี้พิจารณาเมื่อเรากำหนดเซต $S$ ของตัวอย่างข้อมูล $m$ ตัวไว้ สำหรับตัวอย่างข้อมูล $x$ ใด ๆ ถ้าให้ $F_x$ เป็นตัวแปรสุ่มที่มีค่าดังนี้
$$
F_x=\begin{cases}
1&h_S(x)\neq c(x)\\
0&\text{ otherwise }
\end{cases}
$$

เราจะได้ว่าสำหรับ $x\in X-S$

$$
\mathbb{E}[F_x]=\Pr[h(x)\neq c(x_]=\frac{1}{2}
$$

และสำหรับ $x\in S$

$$
\mathbb{E}[F_x]=0
$$

ดังนั้นถ้าให้ $R_c(h_S)$ แทน risk ของ hypothesis $h_S$ เมื่อเทียบกับ concept เป้าหมาย $c$ ที่สุ่มมาจาก $H$ จะได้ว่า

$$
\begin{split}
\mathbb{E}_c[\mathbb{E}_S[R_c(h_S)]]&=\mathbb{E}_S[\mathbb{E}_c[R_c(h_S)]]\\
&=\mathbb{E}_S[\mathbb{E}_c[\frac{1}{d}\sum_{x\in X}F_x]]\\
&=\frac{1}{d}\sum_{i=1}^d\mathbb{E}[F_i]\\
&=\frac{1}{d}\sum_{x_i\in X-S}\frac{1}{2}\\
&=\frac{d-m'}{2d}
\end{split}
$$

ถ้าเรากำหนดให้ $m\leq d/2$ เราจะได้ว่า $m'\leq m\leq d/2$ และ 

$$
\mathbb{E}[R_c(h_S)]\geq \frac{d-(d/2)}{2d}=\frac{1}{4}
$$

สังเกตว่าการที่ $\mathbb{E}[R_c(h_S)]$ จะมีค่าไม่น้อยกว่า $1/4$ ได้นั้น แสดงว่าจะต้องมี concept
