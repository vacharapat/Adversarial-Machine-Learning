{% include lib/mathjax.html %}
# Lower bound

ในหัวข้อก่อนหน้า เราได้เห็นขอบเขตบนของ generalization error หรือ sample complexity ของการเรียนรู้แบบ PAC กันมาแล้ว
ในหัวข้อนี้เราจะมาวิเคราะห์ _ขอบเขตล่าง_ (lower bound) กันดูบ้าง เนื่องจากกรอบการเรียนรู้แบบ PAC นี้ไม่ขึ้นกับการกระจายของข้อมูล กล่าวคือ 
เราสามารถประยุกต์ใช้ผลจากการศึกษาของ PAC ได้โดยไม่จำเป็นต้องทราบว่าการกระจายของข้อมูลที่เราสนใจมีลักษณะเป็นอย่างไร
ขอเพียงเป็นการกระจายที่คงที่แบบใดแบบหนึ่ง ดังนั้นในการหาขอบเขตล่างเราอาจทำได้โดยแสดงให้เห็นการกระจายของข้อมูลที่ generalization error ไม่มีทางต่ำกว่าค่าหนึ่งด้วยความน่าจะเป็นที่มากกว่าศูนย์ หรือในมุมของ sample complexity ก็คือ เราจะแสดงให้เห็นว่ามีการกระจายของข้อมูลที่ เมื่อจำนวนตัวอย่างข้อมูลน้อยกว่าค่าหนึ่ง เรามีโอกาสที่ hypothesis ผลลัพธ์จากการเรียนรู้จะมี risk มากกว่า $\epsilon$ ด้วยความน่าจะเป็นที่มากกว่าศูนย์

## $\Omega(d)$- sample complexity bound
ในขั้นแรกนี้เราจะแสดงตัวอย่างการวิเคราะห์ขอบเขตล่าง โดยจะแสดงให้เห็นว่าจำนวนตัวอย่างข้อมูลที่ต้องการเพื่อเรียนรู้ให้สำเร็จนั้นมีขอบเขตล่างเป็น $\Omega(d)$ เมื่อ $d$ เป็น VC dimenstion ของ hypothesis space ที่เราสนใจ

พิจารณา hypothesis space $H$ ที่มี VC dimension เท่ากับ $d$ ให้ $$X=\{x_1,\dots,x_d\}$$ เป็นเซตของตัวอย่างข้อมูลที่ถูก shatter ได้ด้วย $H$ 
และกำหนดให้ตัวอย่างข้อมูล $x_i$ แต่ละตัวมีความน่าจะเป็นที่จะถูกสุ่มได้เท่ากับ $1/d$ เท่ากันทั้งหมด สังเกตว่าเราสามารถมอง $H$ ให้มีขนาดจำกัดโดยมีจำนวน hypothesis เท่ากับ $2^d$ โดยที่ สำหรับการ label ตัวอย่างข้อมูลใน $X$ แต่ละแบบ จะมี hypothesis ใน $H$ เพียงแบบเดียวเท่านั้นที่สอดคล้องกับการ label ดังกล่าว สังเกตว่าการสุ่มเลือก concept เป้าหมาย $c\in H$ นั้นสามารถทำได้โดยการสุ่มเลือก label ให้กับข้อมูล $x_i\in X$ แต่ละตัว

คราวนี้สมมติให้ concept เป้าหมาย $c$ นั้นถูกสุ่มมาจาก $H$ จากนั้น เมื่อเราสุ่มหยิบตัวอย่างข้อมูลใน $X$ มา $m$ ตัวได้เป็นเซต $$S=\{x_1,\dots,x_{m}\}$$ 
เราจะได้ว่าผลของการดำเนินการนี้จะมีการกระจายเหมือนกับเราสุ่มหยิบตัวอย่างข้อมูล $S$ จาก $X$ มาก่อนจำนวน $m$ ตัวโดยข้อมูลทุกตัวยังไม่ถูก label จากนั้นเราค่อยสุ่ม label $c(x_i)$ ให้กับตัวอย่างข้อมูลแต่ละตัวทั้งที่อยู่ใน $S$ และไม่อยู่ใน $S$ แล้วให้ $c$ เป็น concept ใน $H$ ที่สอดคล้องกับการ label ที่ได้นี้

จากตรงนี้ถ้าเราให้ $h_S\in H$ เป็น hypothesis ที่ consistent กับ $S$ และให้ $R_c(h_S)$ แทน risk ของ $h_S$ เมื่อเทียบกับ concept $c$ จะได้ว่า

$$
\begin{split}
\mathbb{E}_c[\mathbb{E}_S[R_c(h_S)]]&=\mathbb{E}_c[\mathbb{E}_S[\Pr_{x\in X}[h_S(x)\neq c(x)]]]\\
&\geq\mathbb{E}_c[\mathbb{E}_S[\Pr_{x\in X}[x\notin S, h_S(x)\neq c(x)]]]\\
&=\mathbb{E}_c[\mathbb{E}_S[\Pr_{x\in X}[x\notin S]\cdot \Pr[h_S(x)\neq c(x)|x\notin S]]]
\end{split}
$$


ถ้าเรากำหนดให้ $m\leq d/2$ เราจะได้ว่า  $\Pr_{x\in X}[n\notin S]\geq 1/2$ และเนื่องจาก 
$\Pr[h_S(x)\neq c(x)|x\notin S]= 1/2$
เราจึงได้ว่า

$$
\begin{split}
\mathbb{E}_c[\mathbb{E}_S[R_c(h_S)]]&\geq \mathbb{E}_c[\mathbb{E}_S[\frac{1}{2}\cdot\frac{1}{2}]]\\
&=\frac{1}{4}
\end{split}
$$

สังเกตว่าการที่ expectation ของ $$\mathbb{E}_S[R_c(h_S)]$$ บนการสุ่ม concept $c\in H$ จะมีค่าไม่ต่ำกว่า $1/4$ ได้นั้น แสดงว่าจะต้องมี concept $$c^*\in H$$ อย่างน้อยตัวหนึ่งที่ $$\mathbb{E}_S[R_{c^*}(h_S)]\geq 1/4$$ แน่นอน

คราวนี้หากเราพิจารณา concept $c^*$ ดังกล่าว เนื่องจาก

$$
\begin{split}
\mathbb{E}_S[R_{c^*}(h_S)]=\sum_{r=0}^1 r\Pr_S[R_{c^*}(h_S)=r]\\
&\leq \sum_{r\leq 1/8}\frac{1}{8}\Pr[R_{c^*}(h_S)=r] + \sum_{1/8<r\leq 1}1\cdot\Pr[R_{c^*}(h_S)=r]\\
&=\frac{1}{8}\Pr[R_{c^*}(h_S)\leq \frac{1}{8}] + \Pr[R_{c^*}(h_S)> \frac{1}{8}]\\
&\leq \frac{1}{8} + \Pr[R_{c^*}(h_S)> \frac{1}{8}]
\end{split}
$$

ดังนั้นเราจึงได้ว่า

$$
\begin{split}
\Pr[R_{c^*}(h_S)> \frac{1}{8}] +\frac{1}{8}\geq \mathbb{E}_S[R_{c^*}(h_S)]\geq \frac{1}{4}
\end{split}
$$

ดังนั้น

$$
\Pr[R_{c^*}(h_S)> \frac{1}{8}]\geq\frac{1}{8}
$$
