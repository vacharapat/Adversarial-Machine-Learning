{% include lib/mathjax.html %}
# Lower bound

ในหัวข้อก่อนหน้า เราได้เห็นขอบเขตบนของ generalization error หรือ sample complexity ของการเรียนรู้แบบ PAC กันมาแล้ว
ในหัวข้อนี้เราจะมาวิเคราะห์ _ขอบเขตล่าง_ (lower bound) กันดูบ้าง เนื่องจากกรอบการเรียนรู้แบบ PAC นี้ไม่ขึ้นกับการกระจายของข้อมูล กล่าวคือ 
เราสามารถประยุกต์ใช้ผลจากการศึกษาของ PAC ได้โดยไม่จำเป็นต้องทราบว่าการกระจายของข้อมูลที่เราสนใจมีลักษณะเป็นอย่างไร
ขอเพียงเป็นการกระจายที่คงที่แบบใดแบบหนึ่ง ดังนั้นในการหาขอบเขตล่างเราอาจทำได้โดยแสดงให้เห็นการกระจายของข้อมูลที่ generalization error ไม่มีทางต่ำกว่าค่าหนึ่งด้วยความน่าจะเป็นที่มากกว่าศูนย์ หรือในมุมของ sample complexity ก็คือ เราจะแสดงให้เห็นว่ามีการกระจายของข้อมูลที่ เมื่อจำนวนตัวอย่างข้อมูลน้อยกว่าค่าหนึ่ง เรามีโอกาสที่ hypothesis ผลลัพธ์จากการเรียนรู้จะมี risk มากกว่า $\epsilon$ ด้วยความน่าจะเป็นที่มากกว่าศูนย์

## $\Omega(d)$- sample complexity bound
ในขั้นแรกนี้เราจะแสดงตัวอย่างการวิเคราะห์ขอบเขตล่าง โดยจะแสดงให้เห็นว่าจำนวนตัวอย่างข้อมูลที่ต้องการเพื่อเรียนรู้ให้สำเร็จนั้นมีขอบเขตล่างเป็น $\Omega(d)$ เมื่อ $d$ เป็น VC dimenstion ของ hypothesis space ที่เราสนใจ

พิจารณา hypothesis space $H$ ที่มี VC dimension เท่ากับ $d$ ให้ $$X=\{x_1,\dots,x_d\}$$ เป็นเซตของตัวอย่างข้อมูลที่ถูก shatter ได้ด้วย $H$ 
และกำหนดให้ตัวอย่างข้อมูล $x_i$ แต่ละตัวมีความน่าจะเป็นที่จะถูกสุ่มได้เท่ากับ $1/d$ เท่ากันทั้งหมด สังเกตว่าเราสามารถมอง $H$ ให้มีขนาดจำกัดโดยมีจำนวน hypothesis เท่ากับ $2^d$ โดยที่ สำหรับการ label ตัวอย่างข้อมูลใน $X$ แต่ละแบบ จะมี hypothesis ใน $H$ เพียงแบบเดียวเท่านั้นที่สอดคล้องกับการ label ดังกล่าว สังเกตว่าการสุ่มเลือก concept เป้าหมาย $c\in H$ นั้นสามารถทำได้โดยการสุ่มเลือก label ให้กับข้อมูล $x_i$ แต่ละตัว

คราวนี้สมมติให้ concept เป้าหมาย $c$ นั้นถูกสุ่มมาจาก $H$ เมื่อเราสุ่มหยิบตัวอย่างข้อมูลใน $X$ มา $m$ ตัว ให้ $m'$ เป็นจำนวนตัวอย่างข้อมูลที่แตกต่างกันทั้งหมดที่เราได้มา สมมติให้ $$S=\{x_1,\dots,x_{m'}\}$$ เป็นเซตของตัวอย่างข้อมูลดังกล่าว จะเห็นว่าปัญหาของการทำนาย $c$ ที่ถูกต้องนั้นก็คือการทำนาย label ของสมาชิก $x_i\in X-S$ ที่ไม่เห็นใน $S$ ให้ถูกต้องทั้งหมดนั่นเอง ดังนั้นถ้าให้ $h_S\in H$ เป็น hypothesis ผลลัพธ์ของการทำนายที่ consistent กับ $S$ เราจะได้ว่า ถ้าให้ $F_i$ เป็นตัวแปรสุ่มสำหรับตัวอย่างข้อมูล $x_i$ ที่มีค่าดังนี้

$$
F_i=\begin{cases}
1&h_S(x_i)\neq c(x_i)\\
0&\text{ otherwise }
\end{cases}
$$

เราจะได้ว่าสำหรับ $x_i\in X-S$

$$
\mathbb{E}[F_i]=\Pr[h(x_i)\neq c(x_i)]=\frac{1}{2}
$$

และสำหรับ $x_i\in S$

$$
\mathbb{E}[F_i]=0
$$

ดังนั้นถ้าให้ $R_c(h_S)$ แทน risk ของ hypothesis $h_S$ เมื่อเทียบกับ concept เป้าหมาย $c$ ที่สุ่มมาจาก $H$ จะได้ว่า

$$
\begin{split}
\mathbb{E}[R_c(h_S)]&=\mathbb{E}[\frac{1}{d}\sum_{i=1}^dF_i]\\
&=\frac{1}{d}\sum_{i=1}^d\mathbb{E}[F_i]\\
&=\frac{1}{d}\sum_{x_i\in X-S}\frac{1}{2}\\
&=\frac{d-m'}{2d}
\end{split}
$$

ถ้าเรากำหนดให้ $m\leq d/2$ เราจะได้ว่า $m'\leq m\leq d/2$ และ 

$$
\mathbb{E}[R_c(h_S)]\geq \frac{d-(d/2)}{2d}=\frac{1}{4}
$$
